#!/usr/bin/env python3
"""
Backend API Testing for Webmatic.dev MVP - CRITICAL LLM INTEGRATION TEST
Tests the core chat and generation flow after LLM integration fix
"""

import requests
import sys
import json
from datetime import datetime
import os
import time
import uuid

class WebmaticAPITester:
    def __init__(self):
        # Use the public URL from frontend/.env
        self.base_url = "https://webmatic-dev.preview.emergentagent.com/api"
        self.tests_run = 0
        self.tests_passed = 0
        self.created_project_id = None
        self.auth_token = None
        self.test_email = f"testuser_{datetime.now().strftime('%Y%m%d_%H%M%S')}@example.com"

    def log_test(self, name, success, details=""):
        """Log test results"""
        self.tests_run += 1
        if success:
            self.tests_passed += 1
            print(f"âœ… {name} - PASSED {details}")
        else:
            print(f"âŒ {name} - FAILED {details}")

    def test_health(self):
        """Test health endpoint returns ok: true"""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=10)
            if response.status_code == 200:
                data = response.json()
                if data.get("ok") is True and "db" in data:
                    self.log_test("Health Check", True, f"- DB: {data['db']}")
                    return True
                else:
                    self.log_test("Health Check", False, f"- Invalid response: {data}")
            else:
                self.log_test("Health Check", False, f"- Status: {response.status_code}")
        except Exception as e:
            self.log_test("Health Check", False, f"- Error: {str(e)}")
        return False

    def test_auth_register(self):
        """Test user registration"""
        payload = {
            "email": self.test_email,
            "password": "SecurePass123!"
        }
        try:
            response = requests.post(
                f"{self.base_url}/auth/register",
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=10
            )
            if response.status_code == 200:
                data = response.json()
                if "access_token" in data and data.get("token_type") == "bearer":
                    self.auth_token = data["access_token"]
                    self.log_test("Auth Register", True, f"- Token received for {self.test_email}")
                    return True
                else:
                    self.log_test("Auth Register", False, f"- Invalid response: {data}")
            else:
                self.log_test("Auth Register", False, f"- Status: {response.status_code}, Body: {response.text}")
        except Exception as e:
            self.log_test("Auth Register", False, f"- Error: {str(e)}")
        return False

    def test_auth_me(self):
        """Test getting current user info with Bearer token"""
        if not self.auth_token:
            self.log_test("Auth Me", False, "- No auth token available")
            return False
        
        try:
            headers = {"Authorization": f"Bearer {self.auth_token}"}
            response = requests.get(f"{self.base_url}/auth/me", headers=headers, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if "user_id" in data and "email" in data:
                    self.log_test("Auth Me", True, f"- User ID: {data['user_id'][:8]}...")
                    return True
                else:
                    self.log_test("Auth Me", False, f"- Invalid response: {data}")
            else:
                self.log_test("Auth Me", False, f"- Status: {response.status_code}")
        except Exception as e:
            self.log_test("Auth Me", False, f"- Error: {str(e)}")
        return False

    def test_create_project(self):
        """Test project creation with UUID ID"""
        payload = {
            "name": "Homepage Builder Test",
            "description": "Create a simple homepage with hero section and features"
        }
        try:
            response = requests.post(
                f"{self.base_url}/projects",
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=10
            )
            if response.status_code == 200:
                data = response.json()
                if "id" in data and data.get("status") == "created":
                    # Verify UUID format (36 chars with dashes)
                    project_id = data["id"]
                    if len(project_id) == 36 and project_id.count('-') == 4:
                        self.created_project_id = project_id
                        self.log_test("Create Project", True, f"- UUID ID: {project_id[:8]}...")
                        return True
                    else:
                        self.log_test("Create Project", False, f"- Invalid UUID format: {project_id}")
                else:
                    self.log_test("Create Project", False, f"- Invalid response: {data}")
            else:
                self.log_test("Create Project", False, f"- Status: {response.status_code}, Body: {response.text}")
        except Exception as e:
            self.log_test("Create Project", False, f"- Error: {str(e)}")
        return False

    def test_chat_message_persistence(self):
        """Test chat message persistence: POST /api/projects/{id}/chat"""
        if not self.created_project_id:
            self.log_test("Chat Message Persistence", False, "- No project ID available")
            return False
        
        try:
            payload = {"content": "Create a modern homepage with hero section", "role": "user"}
            response = requests.post(
                f"{self.base_url}/projects/{self.created_project_id}/chat",
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=10
            )
            if response.status_code == 200:
                data = response.json()
                if data.get("ok") is True:
                    # Verify message was persisted by retrieving chat
                    get_response = requests.get(f"{self.base_url}/projects/{self.created_project_id}/chat", timeout=10)
                    if get_response.status_code == 200:
                        chat_data = get_response.json()
                        messages = chat_data.get("messages", [])
                        if len(messages) > 0 and messages[-1].get("content") == payload["content"]:
                            self.log_test("Chat Message Persistence", True, f"- Message persisted and retrieved")
                            return True
                        else:
                            self.log_test("Chat Message Persistence", False, f"- Message not found in chat history")
                    else:
                        self.log_test("Chat Message Persistence", False, f"- Failed to retrieve chat: {get_response.status_code}")
                else:
                    self.log_test("Chat Message Persistence", False, f"- Invalid response: {data}")
            else:
                self.log_test("Chat Message Persistence", False, f"- Status: {response.status_code}, Body: {response.text}")
        except Exception as e:
            self.log_test("Chat Message Persistence", False, f"- Error: {str(e)}")
        return False

    def test_code_generation_llm(self):
        """MOST CRITICAL: Test code generation with LLM integration - Review Request Priority Test"""
        if not self.created_project_id or not self.auth_token:
            self.log_test("Code Generation LLM", False, "- Missing project ID or auth token")
            return False
        
        try:
            # Use the exact prompt from review request
            payload = {"provider": "claude", "prompt": "Create a professional homepage for Webmatic.dev with hero section, features, pricing, testimonials"}
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.auth_token}"
            }
            
            print(f"    ğŸ”„ Calling LLM generation with complex prompt (may take 10-30 seconds)...")
            response = requests.post(
                f"{self.base_url}/projects/{self.created_project_id}/generate",
                json=payload,
                headers=headers,
                timeout=60  # Extended timeout for LLM calls
            )
            
            if response.status_code == 200:
                data = response.json()
                mode = data.get("mode")
                files = data.get("files", [])
                html_preview = data.get("html_preview", "")
                error = data.get("error")
                
                print(f"    ğŸ“Š Response: mode={mode}, files={len(files)}, error={error}")
                
                # Check if it's using AI mode instead of STUB mode
                if mode == "ai":
                    if error is None:
                        if len(files) > 0 and html_preview:
                            # Verify it's not just stub content
                            is_stub = "Auto-generated preview" in html_preview and "Refine via chat" in html_preview
                            has_webmatic_context = "Webmatic" in html_preview or "webmatic" in html_preview
                            
                            if not is_stub and has_webmatic_context:
                                self.log_test("Code Generation LLM", True, 
                                            f"- AI mode successful, {len(files)} files, contextual Webmatic.dev content, no stub fallback")
                                return True
                            elif not is_stub:
                                self.log_test("Code Generation LLM", True, 
                                            f"- AI mode successful, {len(files)} files, real LLM content (minor: no Webmatic context)")
                                return True
                            else:
                                self.log_test("Code Generation LLM", False, 
                                            f"- AI mode but still returning stub content: 'Auto-generated preview. Refine via chat on the left.'")
                        else:
                            self.log_test("Code Generation LLM", False, 
                                        f"- AI mode but missing files ({len(files)}) or html_preview")
                    else:
                        self.log_test("Code Generation LLM", False, 
                                    f"- AI mode but has error: {error}")
                elif mode == "stub":
                    self.log_test("Code Generation LLM", False, 
                                f"- CRITICAL: Falling back to STUB mode, LLM integration not working. Error: {error}")
                else:
                    self.log_test("Code Generation LLM", False, 
                                f"- Unknown mode: {mode}")
            else:
                self.log_test("Code Generation LLM", False, 
                            f"- Status: {response.status_code}, Body: {response.text}")
        except Exception as e:
            self.log_test("Code Generation LLM", False, f"- Error: {str(e)}")
        return False

    def test_scaffold_project(self):
        if not self.created_project_id:
            self.log_test("Scaffold Project", False, "- No project ID available")
            return False
        
        try:
            payload = {"provider": "claude", "model": "claude-4-sonnet"}
            response = requests.post(
                f"{self.base_url}/projects/{self.created_project_id}/scaffold", 
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=30  # Increased timeout for LLM calls
            )
            if response.status_code == 200:
                data = response.json()
                if data.get("status") == "planned" and data.get("plan"):
                    plan = data["plan"]
                    frontend_count = len(plan.get("frontend", []))
                    backend_count = len(plan.get("backend", []))
                    database_count = len(plan.get("database", []))
                    self.log_test("Scaffold Project", True, 
                                f"- Status: planned, Frontend: {frontend_count}, Backend: {backend_count}, DB: {database_count}")
                    return True
                else:
                    self.log_test("Scaffold Project", False, f"- Invalid plan: status={data.get('status')}, plan={bool(data.get('plan'))}")
            else:
                self.log_test("Scaffold Project", False, f"- Status: {response.status_code}, Body: {response.text}")
        except Exception as e:
            self.log_test("Scaffold Project", False, f"- Error: {str(e)}")
        return False

    def test_runs_list_with_quality_score(self):
        """Test runs list includes quality_score for latest run"""
        if not self.created_project_id:
            self.log_test("Runs List Quality Score", False, "- No project ID available")
            return False
        
        try:
            response = requests.get(f"{self.base_url}/projects/{self.created_project_id}/runs", timeout=10)
            if response.status_code == 200:
                data = response.json()
                if isinstance(data, list) and len(data) > 0:
                    latest_run = data[0]  # Should be sorted by created_at desc
                    required_fields = ["provider", "model", "mode", "plan_counts", "quality_score"]
                    missing_fields = [f for f in required_fields if f not in latest_run]
                    
                    if not missing_fields and latest_run.get("quality_score") is not None:
                        self.log_test("Runs List Quality Score", True, 
                                    f"- Quality Score: {latest_run['quality_score']}, Provider: {latest_run['provider']}")
                        return True
                    else:
                        self.log_test("Runs List Quality Score", False, f"- Missing fields: {missing_fields}")
                else:
                    self.log_test("Runs List Quality Score", False, f"- No runs found or invalid format")
            else:
                self.log_test("Runs List Quality Score", False, f"- Status: {response.status_code}")
        except Exception as e:
            self.log_test("Runs List Quality Score", False, f"- Error: {str(e)}")
        return False

    def test_compare_providers(self):
        """Test compare providers creates two run records with baseline, variants, diff"""
        if not self.created_project_id:
            self.log_test("Compare Providers", False, "- No project ID available")
            return False
        
        try:
            response = requests.post(
                f"{self.base_url}/projects/{self.created_project_id}/compare-providers",
                headers={"Content-Type": "application/json"},
                timeout=60  # Longer timeout for multiple LLM calls
            )
            if response.status_code == 200:
                data = response.json()
                required_keys = ["baseline", "variants", "diff"]
                missing_keys = [k for k in required_keys if k not in data]
                
                if not missing_keys:
                    baseline = data["baseline"]
                    variants = data["variants"]
                    diff = data["diff"]
                    
                    # Verify structure
                    baseline_valid = "provider" in baseline and "model" in baseline and "plan" in baseline
                    variants_valid = isinstance(variants, list) and len(variants) > 0
                    diff_valid = isinstance(diff, dict) and all(k in diff for k in ["frontend", "backend", "database"])
                    
                    if baseline_valid and variants_valid and diff_valid:
                        self.log_test("Compare Providers", True, 
                                    f"- Baseline: {baseline['provider']}, Variants: {len(variants)}, Diff sections: {len(diff)}")
                        return True
                    else:
                        self.log_test("Compare Providers", False, f"- Invalid structure: baseline={baseline_valid}, variants={variants_valid}, diff={diff_valid}")
                else:
                    self.log_test("Compare Providers", False, f"- Missing keys: {missing_keys}")
            else:
                self.log_test("Compare Providers", False, f"- Status: {response.status_code}, Body: {response.text}")
        except Exception as e:
            self.log_test("Compare Providers", False, f"- Error: {str(e)}")
        return False

    def test_json_parsing_long_prompt(self):
        """Test JSON parsing with long, detailed prompts that previously caused truncation"""
        if not self.created_project_id or not self.auth_token:
            self.log_test("JSON Parsing Long Prompt", False, "- Missing project ID or auth token")
            return False
        
        try:
            # Moderately long prompt that tests JSON parsing without being excessive
            long_prompt = """Create a professional business website for 'TechFlow Solutions' with these sections:
            
            1. Hero Section: Modern headline about digital transformation, compelling subheadline, call-to-action button
            2. Services: Web Development, Mobile Apps, Cloud Solutions, AI Integration - each with descriptions
            3. About: Company overview and mission statement
            4. Contact: Contact form and company information
            
            Make it responsive with modern CSS, clean typography, and professional styling."""
            
            payload = {"provider": "claude", "prompt": long_prompt}
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.auth_token}"
            }
            
            print(f"    ğŸ”„ Testing JSON parsing with detailed prompt ({len(long_prompt)} chars)...")
            response = requests.post(
                f"{self.base_url}/projects/{self.created_project_id}/generate",
                json=payload,
                headers=headers,
                timeout=60
            )
            
            if response.status_code == 200:
                try:
                    data = response.json()  # This will fail if JSON is malformed
                    mode = data.get("mode")
                    files = data.get("files", [])
                    html_preview = data.get("html_preview", "")
                    error = data.get("error")
                    
                    print(f"    ğŸ“Š Detailed prompt response: mode={mode}, files={len(files)}, html_len={len(html_preview)}, error={error}")
                    
                    if mode == "ai" and error is None:
                        # Check for content - more lenient criteria
                        if len(files) > 0 or len(html_preview) > 0:
                            has_techflow = "TechFlow" in html_preview or "techflow" in html_preview.lower()
                            
                            if has_techflow:
                                self.log_test("JSON Parsing Long Prompt", True, 
                                            f"- JSON parsed successfully, {len(files)} files, {len(html_preview)} chars, contextual content")
                                return True
                            elif len(html_preview) > 500:
                                self.log_test("JSON Parsing Long Prompt", True, 
                                            f"- JSON parsed successfully, {len(files)} files, {len(html_preview)} chars content")
                                return True
                            else:
                                self.log_test("JSON Parsing Long Prompt", False, 
                                            f"- JSON parsed but minimal content ({len(html_preview)} chars)")
                        else:
                            self.log_test("JSON Parsing Long Prompt", False, 
                                        f"- JSON parsed but no content generated")
                    else:
                        self.log_test("JSON Parsing Long Prompt", False, 
                                    f"- JSON parsed but mode={mode}, error={error}")
                        
                except json.JSONDecodeError as je:
                    self.log_test("JSON Parsing Long Prompt", False, 
                                f"- JSON parsing failed: {str(je)}")
            else:
                self.log_test("JSON Parsing Long Prompt", False, 
                            f"- Status: {response.status_code}, Body: {response.text[:200]}...")
        except Exception as e:
            self.log_test("JSON Parsing Long Prompt", False, f"- Error: {str(e)}")
        return False
        """Test DELETE /api/projects/{id} - successful deletion"""
        if not self.created_project_id:
            self.log_test("Delete Project Success", False, "- No project ID available")
            return False
        
        try:
            # First verify project exists
            get_response = requests.get(f"{self.base_url}/projects/{self.created_project_id}", timeout=10)
            if get_response.status_code != 200:
                self.log_test("Delete Project Success", False, f"- Project doesn't exist before deletion: {get_response.status_code}")
                return False
            
            # Delete the project
            response = requests.delete(f"{self.base_url}/projects/{self.created_project_id}", timeout=10)
            if response.status_code == 200:
                data = response.json()
                if data.get("ok") is True and "message" in data:
                    # Verify project no longer exists
                    verify_response = requests.get(f"{self.base_url}/projects/{self.created_project_id}", timeout=10)
                    if verify_response.status_code == 404:
                        self.log_test("Delete Project Success", True, 
                                    f"- Project deleted successfully, returns 404 on subsequent GET")
                        return True
                    else:
                        self.log_test("Delete Project Success", False, 
                                    f"- Project still exists after deletion: {verify_response.status_code}")
                else:
                    self.log_test("Delete Project Success", False, f"- Invalid response: {data}")
            else:
                self.log_test("Delete Project Success", False, f"- Status: {response.status_code}, Body: {response.text}")
        except Exception as e:
            self.log_test("Delete Project Success", False, f"- Error: {str(e)}")
        return False

    def test_delete_nonexistent_project(self):
        """Test DELETE /api/projects/{id} - non-existent project returns 404"""
        fake_project_id = str(uuid.uuid4())
        
        try:
            response = requests.delete(f"{self.base_url}/projects/{fake_project_id}", timeout=10)
            if response.status_code == 404:
                self.log_test("Delete Nonexistent Project", True, 
                            f"- Correctly returns 404 for non-existent project")
                return True
            else:
                self.log_test("Delete Nonexistent Project", False, 
                            f"- Expected 404, got {response.status_code}: {response.text}")
        except Exception as e:
            self.log_test("Delete Nonexistent Project", False, f"- Error: {str(e)}")
        return False

    def test_data_cleanup_verification(self):
        """Test that related chats and runs are cleaned up after project deletion"""
        # Create a new project for this test since we deleted the previous one
        payload = {
            "name": "Cleanup Test Project",
            "description": "Test project for verifying data cleanup"
        }
        
        try:
            # Create project
            response = requests.post(
                f"{self.base_url}/projects",
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=10
            )
            if response.status_code != 200:
                self.log_test("Data Cleanup Verification", False, f"- Failed to create test project: {response.status_code}")
                return False
            
            test_project_id = response.json()["id"]
            
            # Add a chat message to create related data
            chat_payload = {"content": "Test message for cleanup", "role": "user"}
            chat_response = requests.post(
                f"{self.base_url}/projects/{test_project_id}/chat",
                json=chat_payload,
                headers={"Content-Type": "application/json"},
                timeout=10
            )
            
            # Create a run by scaffolding (this creates run data)
            scaffold_payload = {"provider": "claude", "model": "claude-4-sonnet"}
            scaffold_response = requests.post(
                f"{self.base_url}/projects/{test_project_id}/scaffold",
                json=scaffold_payload,
                headers={"Content-Type": "application/json"},
                timeout=30
            )
            
            # Verify chat and runs exist before deletion
            chat_get = requests.get(f"{self.base_url}/projects/{test_project_id}/chat", timeout=10)
            runs_get = requests.get(f"{self.base_url}/projects/{test_project_id}/runs", timeout=10)
            
            chat_exists_before = chat_get.status_code == 200 and len(chat_get.json().get("messages", [])) > 0
            runs_exist_before = runs_get.status_code == 200 and len(runs_get.json()) > 0
            
            # Delete the project
            delete_response = requests.delete(f"{self.base_url}/projects/{test_project_id}", timeout=10)
            if delete_response.status_code != 200:
                self.log_test("Data Cleanup Verification", False, f"- Failed to delete project: {delete_response.status_code}")
                return False
            
            # Verify related data is cleaned up
            chat_get_after = requests.get(f"{self.base_url}/projects/{test_project_id}/chat", timeout=10)
            runs_get_after = requests.get(f"{self.base_url}/projects/{test_project_id}/runs", timeout=10)
            
            # Chat and runs endpoints should return empty or 404 after project deletion
            chat_cleaned = chat_get_after.status_code == 404 or (chat_get_after.status_code == 200 and len(chat_get_after.json().get("messages", [])) == 0)
            runs_cleaned = runs_get_after.status_code == 200 and len(runs_get_after.json()) == 0
            
            if chat_cleaned and runs_cleaned:
                self.log_test("Data Cleanup Verification", True, 
                            f"- Related data cleaned up successfully (chat: {chat_exists_before}â†’cleaned, runs: {runs_exist_before}â†’cleaned)")
                return True
            else:
                self.log_test("Data Cleanup Verification", False, 
                            f"- Data cleanup failed (chat cleaned: {chat_cleaned}, runs cleaned: {runs_cleaned})")
                
        except Exception as e:
            self.log_test("Data Cleanup Verification", False, f"- Error: {str(e)}")
        return False

    def run_all_tests(self):
        """Run all backend tests focusing on LLM Integration Quality and JSON Parsing Fix"""
        print("ğŸš€ CRITICAL LLM INTEGRATION TEST - Webmatic.dev Backend")
        print(f"ğŸ“¡ Testing against: {self.base_url}")
        print("ğŸ¯ Focus: LLM Integration Quality and JSON Parsing Fix")
        print("ğŸ” Review Request: Verify AI mode, content quality, JSON parsing, error recovery")
        print("=" * 70)

        # Test sequence based on review request priorities
        tests = [
            ("1. Health Check", self.test_health),
            ("2. Auth Register", self.test_auth_register),
            ("3. Auth Me with Bearer Token", self.test_auth_me),
            ("4. Project Creation", self.test_create_project),
            ("5. Chat Message Persistence", self.test_chat_message_persistence),
            ("6. ğŸ”¥ CRITICAL: Code Generation LLM", self.test_code_generation_llm),
            ("7. ğŸ”¥ CRITICAL: JSON Parsing Long Prompt", self.test_json_parsing_long_prompt),
        ]

        for test_name, test_func in tests:
            print(f"\nğŸ” Running {test_name}...")
            test_func()

        # Summary
        print("\n" + "=" * 70)
        print(f"ğŸ“Š Test Results: {self.tests_passed}/{self.tests_run} passed")
        
        if self.tests_passed == self.tests_run:
            print("ğŸ‰ ALL CRITICAL LLM INTEGRATION TESTS PASSED!")
            print("âœ… System no longer falls back to stub mode")
            print("âœ… JSON parsing handles long prompts without truncation")
            print("âœ… Content quality is professional and contextual")
            return True
        else:
            print("âš ï¸  CRITICAL LLM INTEGRATION TESTS FAILED!")
            print("âŒ LLM integration or JSON parsing issues detected")
            return False

def main():
    tester = WebmaticAPITester()
    success = tester.run_all_tests()
    return 0 if success else 1

if __name__ == "__main__":
    sys.exit(main())